# Initial Thoughts
***
- Goal: English -> Korean skills, but with below anti-LLM skill 
- Done: making a skill file in `.opencode/skills/`
- Source/Context: reference another skills format. 

# TO-DOs
***
- [x] Pick skill identity: `translation-en-ko` (default) + define target output (docs vs UI microcopy)
- [x] Copy repo skill conventions (frontmatter + section style) from `.opencode/skills/research/SKILL.md` and `.opencode/skills/legal-work/SKILL.md`
- [x] Define translation invariants: preserve meaning, numbers/units, formatting; protect code/inline-code/paths/URLs/placeholders
- [ ] Design workflow: protect spans -> first-pass translation (fidelity) -> anti-LLM naturalization pass (style-only) -> QA
- [ ] Write edge-case rules: Markdown links/tables, quoted UI strings, proper nouns, technical terms, and glossary consistency
- [ ] Create `.opencode/skills/translation-en-ko/SKILL.md` and validate with 3 sample inputs (markdown + code + UI)

# Reading & Provocation
***

## Reading

Repo skill format here is simple: skills live at `.opencode/skills/<name>/SKILL.md` and include YAML frontmatter with at least `name` + `description` (see `.opencode/skills/research/SKILL.md`, `.opencode/skills/reference-find/SKILL.md`). Most skills then enforce a clear contract via sections like `## Required Inputs`, `## Workflow`, `## Output Format`, `## Guardrails` (see `.opencode/skills/legal-work/SKILL.md`, `.opencode/skills/meeting-prep/SKILL.md`). Some optionally include extra frontmatter like `license` (see `.opencode/skills/pptx/SKILL.md`).

You already drafted the core anti-LLM heuristics inside this note under `# Anti-LLM Skill` (same file: `4-Quick-Note/번역 스킬 만들기.md`). The key adaptation for translation: apply anti-LLM as a *style-only naturalization pass* that removes translationese/AI cadence while explicitly forbidding new claims, new emphasis, or new author voice.

Proposed minimal skeleton (match house style; keep it tight):

```markdown
---
name: translation-en-ko
description: Translate English to natural Korean while preserving meaning/formatting, then remove translationese/LLM cadence without adding new content.
---

# English -> Korean Translation

## Required Inputs

## Workflow

## Output Format

## Guardrails

## Edge Cases (Markdown, Code, UI Strings)
```

## Provocation

- Creativity: Write 3 different "natural Korean" targets (dev blog / internal spec / UI microcopy) and list what changes between them.
- Critical-thinking: List 10 things the translator must never change (claims, numbers, certainty, formatting, etc.). Circle the top 3 that are easiest to accidentally break during anti-LLM rewriting.
- Memory: From your `# Anti-LLM Skill`, pick 5 rules that are safe for translation and 5 that are risky (because they can change meaning). Explain why, in one sentence each.
- Meta-cognition: Define a pass/fail QA checklist for the output (5 checks). Then define 2 "red flag" checks that trigger asking the user a clarifying question.

# Annotate & Note-Taking
***
The workflow should follow this form: 
1. Just translate it. 
2. Then, apply anti-LLM pattern to make it more natural and human-like. 

# Closing
***
> `daily-agent` prompt: 


# Anti-LLM Skill
***
```markdown
---
name: anti-llm-writing
version: 1.0.0
description: |
  Strip the "LLM voice" from text. Goes beyond surface-level cleanup (which
  the humanizer skill handles) to attack the deeper problem: every major LLM
  has a recognizable cadence, sentence rhythm, and structural habit that
  makes text feel machine-written even when individual words are fine.
  Works for both English and Korean text.
allowed-tools:
  - Read
  - Write
  - Edit
  - Grep
  - Glob
  - AskUserQuestion
---

# Anti-LLM Writing: Kill the AI Voice

The humanizer skill catches individual red-flag words and phrases. This skill targets the **structural and tonal fingerprint** that makes text sound like it came from a specific LLM, even after surface cleanup.

The core insight: AI text is detectable not because of specific words, but because of **consistent rhythm, predictable structure, and uniform emotional temperature.**

## The Problem

Every major LLM has a voice:
- **ChatGPT**: Enthusiastic explainer. Loves "Let's dive in", "Here's the thing", em dashes for dramatic pauses, and wrapping up with optimistic conclusions.
- **Claude**: Measured academic. "It's worth noting that", balanced on-the-other-hand structures, and careful hedging.
- **Gemini**: Corporate presenter. Clean bullet points, "Key takeaways", and systematic formatting.

A human who reads a lot of AI text can spot the voice even if every individual sentence passes the humanizer check. The problem is the **aggregate pattern**, not any single sentence.

---

## DETECTION CHECKLIST

Before editing, scan the full text for these structural tells:

### 1. Rhythm Monotony
- [ ] Are most sentences roughly the same length?
- [ ] Does every paragraph have 2-4 sentences?
- [ ] Is there a predictable pattern (short intro → explanation → example)?
- [ ] Does every section follow the same internal structure?

### 2. Emotional Flatline
- [ ] Is the emotional tone constant throughout?
- [ ] Are there zero moments of frustration, excitement, doubt, or humor?
- [ ] Does every claim feel equally weighted?
- [ ] Is the author invisible — could anyone have written this?

### 3. Structural Predictability
- [ ] Does the piece follow a textbook outline? (Problem → Solution → Benefits → Conclusion)
- [ ] Are transitions too smooth? ("Now let's look at...", "This brings us to...")
- [ ] Do all lists have 3 items? (Rule of three)
- [ ] Is every section roughly the same length?

### 4. Formatting Over-Engineering
- [ ] Is bold used mechanically on every key term?
- [ ] Are there parenthetical translations/definitions everywhere? ("Deterministic Retrieval (결정론적 검색)")
- [ ] Do bullet points all start with bold headers followed by colons?
- [ ] Are technical terms always accompanied by explanations?

### 5. Korean-Specific Tells (국문 전용)
- [ ] 전체가 "~합니다/습니다" 경어체로 통일되어 있는가?
- [ ] "~이자, ~인 동시에" 같은 수식 구조가 과도한가?
- [ ] "~를 제공합니다", "~를 수행합니다" 같은 기업 보도자료 어투가 있는가?
- [ ] "여기가 핵심입니다", "구체적으로 살펴보겠습니다" 같은 AI 전환구가 있는가?
- [ ] 한국어 문장인데 영어 병기가 과도한가? ("결정론적 검색(Deterministic Retrieval)")
- [ ] 실제 한국 개발자 블로그 말투와 동떨어져 있는가?

---

## REWRITE RULES

### Rule 1: Break the Rhythm

**Bad** (every sentence is 15-20 words):
> CoMeT treats memory like a human brain. It uses a fast layer for immediate context. It also maintains a structured tree for long-term recall. This combination allows efficient retrieval.

**Good** (varied):
> The structure is simple. A fast layer for immediate reference, a tree for long-term storage. Two layers.

Techniques:
- Use sentence fragments. On purpose.
- Follow a long sentence with a brutally short one.
- Let some paragraphs be one sentence. Let others be five.
- Occasionally start with "And" or "But."

### Rule 2: Show the Author

**Bad** (invisible author):
> This approach offers several advantages over traditional methods.

**Good** (someone is here):
> I got sick of picking between "expensive and smart" or "cheap and dumb."

Techniques:
- Use "I" when the author has a perspective.
- Express genuine opinions, not just facts.
- Allow doubt: "I'm not sure this scales past 10k nodes yet."
- Have a take. Humans have takes.

### Rule 3: Kill Smooth Transitions

**Bad**:
> Now let's look at how CoMeT actually processes data. To understand this, consider the following example.

**Good**:
> One example makes it click.

Or just... don't transition. Jump. Readers can follow.

### Rule 4: Vary Emotional Temperature

**Bad** (flatline):
> The results were promising. The system achieved good accuracy. The cost savings were notable.

**Good** (has a pulse):
> CoMeT RAG matches Full Context accuracy at 5.2x lower cost. Session mode is 13.5x cheaper and missed one question.

No adjectives telling the reader how to feel. Let the numbers hit.

### Rule 5: Asymmetric Structure

**Bad** (every section looks the same):
> ### Feature A
> Description. Benefit. Example.
> ### Feature B
> Description. Benefit. Example.
> ### Feature C
> Description. Benefit. Example.

**Good**:
> ### Feature A
> Two sentences.
> ### Feature B
> A paragraph with an example embedded mid-sentence, then a one-liner that lands.
> ### Feature C
> Just the example. No preamble.

### Rule 6: Kill Formatting Crutches

- Don't bold every technical term. Bold is emphasis, not a glossary.
- Don't parenthetically define terms the audience already knows.
- Don't force bullet points when a sentence works.
- If you have 3 items in every list, you're doing it wrong.

### Rule 7: Korean-Specific Fixes (국문)

**문체를 섞어라:**
- 전체를 "~합니다"로 쓰지 말고, 해라체("~이다"), 해요체("~해요"), 구어체("~거든")를 문맥에 맞게 섞어라.
- 개발자 블로그라면 반말이 자연스럽다. 기술 문서가 아닌 이상 경어체 일변도는 AI 냄새가 난다.

**번역투를 죽여라:**
- "~를 제공합니다" → "~가 있다" 또는 "~을 쓸 수 있다"
- "~를 수행합니다" → "~한다"
- "~에 대한 접근 방식" → "~하는 법"
- "이것이 핵심 novelty입니다" → "여기가 핵심이다"

**영어 병기를 최소화해라:**
- 독자가 이미 아는 용어는 한국어만 쓴다.
- 처음 나오는 전문 용어만 한 번 병기하고 이후로는 한국어만.
- "(Deterministic Retrieval)" 같은 괄호 번역은 웬만하면 삭제.

---

## PROCESS

1. **Read the full text** — don't start editing line by line. Get the overall feel.
2. **Run the detection checklist** — mark which structural patterns are present.
3. **Identify the "LLM voice"** — is it ChatGPT-enthusiastic? Claude-measured? Gemini-systematic?
4. **Rewrite for asymmetry** — break rhythm, vary section lengths, remove smooth transitions.
5. **Inject the author** — add perspective, opinion, doubt where appropriate.
6. **Strip formatting crutches** — reduce bold, remove unnecessary parenthetical definitions.
7. **Read aloud** — if it sounds like a presentation, rewrite. If it sounds like someone talking at a bar about their project, you're close.

## OUTPUT

Provide the rewritten text. Optionally note the major patterns you broke.

---

## FULL EXAMPLE (Korean)

**Before (AI voice — formally consistent, structurally predictable):**
> ### 3. 무손실 계층 구조: 요약과 RAG의 경계가 사라지다
>
> 이것이 CoMeT의 핵심 novelty입니다. 일반적인 AI 아키텍처는 전체 흐름을 파악하기 위한 '요약'과 특정 사실을 찾기 위한 'RAG'를 별개의 프로세스로 다룹니다. 하지만 CoMeT는 이 경계를 완전히 허물었습니다.
>
> MemoryNode는 과거의 기록임과 동시에 검색을 위한 인덱스 역할을 동시에 수행합니다.
> - **Summary**: 맥락 파악을 위한 요약본이자, 시맨틱 검색을 위한 인덱스(WHAT).
> - **Trigger**: 이 정보가 *언제* 소환되어야 하는지 정의하는 트리거(WHEN).
> - **Raw Data**: 완벽한 정확도를 위해 보존되는 원본 기록(Raw).

**After (human voice — rhythm breaks, no AI transitions):**
> ### 3. 요약이 곧 인덱스다
>
> 보통은 '요약'과 'RAG 검색'이 별개 파이프라인이다. CoMeT는 이걸 합쳤다.
>
> MemoryNode 하나가 과거 기록이면서 동시에 검색 인덱스다.
> - **Summary**: 맥락 파악 + 시맨틱 검색 진입점.
> - **Trigger**: 이 정보를 *언제* 꺼내야 하는지.
> - **Raw Data**: 원본 그대로.

**Changes made:**
- "이것이 CoMeT의 핵심 novelty입니다" (AI transition) → removed entirely
- "일반적인 AI 아키텍처는 ~를 별개의 프로세스로 다룹니다" (번역투) → "보통은 ~이 별개 파이프라인이다"
- "이 경계를 완전히 허물었습니다" (promotional) → "이걸 합쳤다"
- "과거의 기록임과 동시에 검색을 위한 인덱스 역할을 동시에 수행합니다" (과도한 수식) → "과거 기록이면서 동시에 검색 인덱스다"
- 괄호 영어 병기 (WHAT), (WHEN), (Raw) 삭제
- "완벽한 정확도를 위해 보존되는" → "원본 그대로"
- 습니다 → 이다/~다 (해라체로 전환)

---

## FULL EXAMPLE (English)

**Before (AI voice):**
> CoMeT treats memory like a human brain: a fast layer for immediate context and a structured tree for long-term recall.
>
> ### 1. Dual-speed architecture
>
> CoMeT doesn't just process text; it "senses" it.
> - **The Sensor (Fast Layer)**: Uses a small model (like GPT-4o-mini) to track "cognitive load." It watches for topic shifts or high information density in real-time.

**After (human voice):**
> The structure is simple. A fast layer for immediate reference, a tree for long-term storage. Two layers.
>
> ### 1. Dual-Speed Architecture
>
> CoMeT doesn't just store text. It reads the room.
> - **Sensor**: A cheap model (GPT-4o-mini) monitors the conversation in real-time. Catches topic shifts and information density spikes.

**Changes made:**
- "treats memory like a human brain" (cliché analogy) → stated the structure directly
- "doesn't just X; it Y" (negative parallelism) → simple statement
- "(Fast Layer)" label removed — unnecessary categorization
- "Uses a small model (like GPT-4o-mini) to track" → "A cheap model (GPT-4o-mini) monitors" — more direct
- "high information density" → "information density spikes" — shorter, punchier
```
